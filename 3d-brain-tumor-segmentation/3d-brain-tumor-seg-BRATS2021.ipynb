{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Microsoft Computer Vision Accelerator for Business Solutions__\n",
    "# Brain tumor 3D segmentation with AzureML and MONAI (BRATS21)\n",
    "\n",
    "Glioma brain tumors are among the most aggressive and lethal types of brain tumors. They can cause a range of symptoms, including headaches, seizures, and difficulty with speech and movement. Gliomas can be difficult to diagnose and treat, and early detection is critical for improving patient outcomes.\n",
    "\n",
    "Computer vision AI has emerged as a promising tool for supporting the diagnosis and treatment of glioma brain tumors. AI algorithms can analyze medical images of the brain and identify the location and extent of tumors with a high degree of accuracy. This can help clinicians make more informed decisions about treatment options, such as surgery or radiation therapy, and monitor the progress of the disease over time. Additionally, AI algorithms can help researchers better understand the underlying biology of gliomas and develop new therapies for this challenging disease.\n",
    "\n",
    "This demo is based on the [MONAI 3d brain tumor segmentation tutorial](https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/swin_unetr_brats21_segmentation_3d.ipynb) and shows how to construct a training workflow of multi-labels segmentation task.\n",
    "\n",
    "The sub-regions considered for evaluation in the BraTS 21 challenge are the \"enhancing tumor\" (ET), the \"tumor core\" (TC), and the \"whole tumor\" (WT). The ET is described by areas that show hyper-intensity in T1Gd when compared to T1, but also when compared to “healthy” white matter in T1Gd. The TC describes the bulk of the tumor, which is what is typically resected. The TC entails the ET, as well as the necrotic (NCR) parts of the tumor. The appearance of NCR is typically hypo-intense in T1-Gd when compared to T1. The WT describes the complete extent of the disease, as it entails the TC and the peritumoral edematous/invaded tissue (ED), which is typically depicted by the hyper-intense signal in FLAIR [BraTS 21].\n",
    "\n",
    "![image](./media/fig_brats21.png)\n",
    "\n",
    "This notebook has been developed and tested with VSCode connected to an AzureML `STANDARD_D13_V2` Compute Instance using the `azureml_py310_sdkv2` kernel.\n",
    "\n",
    "## References\n",
    "\n",
    "[1]: Hatamizadeh, A., Nath, V., Tang, Y., Yang, D., Roth, H. and Xu, D., 2022. Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images. arXiv preprint arXiv:2201.01266.\n",
    "\n",
    "[2]: Tang, Y., Yang, D., Li, W., Roth, H.R., Landman, B., Xu, D., Nath, V. and Hatamizadeh, A., 2022. Self-supervised pre-training of swin transformers for 3d medical image analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 20730-20740).\n",
    "\n",
    "[3] U.Baid, et al., The RSNA-ASNR-MICCAI BraTS 2021 Benchmark on Brain Tumor Segmentation and Radiogenomic Classification, arXiv:2107.02314, 2021.\n",
    "\n",
    "[4] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby, et al. \"The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)\", IEEE Transactions on Medical Imaging 34(10), 1993-2024 (2015) DOI: 10.1109/TMI.2014.2377694\n",
    "\n",
    "[5] S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J.S. Kirby, et al., \"Advancing The Cancer Genome Atlas glioma MRI collections with expert segmentation labels and radiomic features\", Nature Scientific Data, 4:170117 (2017) DOI: 10.1038/sdata.2017.117\n",
    "\n",
    "[6] S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J. Kirby, et al., \"Segmentation Labels and Radiomic Features for the Pre-operative Scans of the TCGA-GBM collection\", The Cancer Imaging Archive, 2017. DOI: 10.7937/K9/TCIA.2017.KLXWJJ1Q\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installs and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on azureml_py310_sdkv2 kernel\n",
    "# %pip install torch==1.12.0 torchvision==0.13.0 torchaudio==0.12.0\n",
    "# %pip install 'monai[nibabel, ignite, tqdm]'\n",
    "# %pip install itkwidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import base64\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "from itkwidgets import view\n",
    "from ipywidgets import interact\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml import MLClient, command, Input\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml.entities import ManagedOnlineEndpoint, ManagedOnlineDeployment, Model, Environment, JobService, Data, CodeConfiguration, OnlineRequestSettings, AmlCompute\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "\n",
    "from monai.apps import DecathlonDataset\n",
    "from monai.data import DataLoader, Dataset\n",
    "from monai.transforms import Compose, LoadImaged, EnsureChannelFirstd, EnsureTyped, Orientationd, Spacingd, NormalizeIntensityd, MapTransform\n",
    "from monai.visualize.utils import blend_images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define central variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AzureML Workspace\n",
    "subscription_id = '<your Azure subscription id>'\n",
    "resource_group = '<your Azure resource group name>'\n",
    "workspace = '<your AzureML Workspace name'\n",
    "\n",
    "# Training\n",
    "experiment = 'brain-tumor-segmentation' # AzureML experiment name\n",
    "dataset_name=\"BRATS2021\"\n",
    "train_target = 'NC96adsA100'\n",
    "\n",
    "# Deployment\n",
    "online_endpoint_name = \"brain-tumor-seg-brats21\"\n",
    "registered_model_name = 'BRATS21'\n",
    "deployment_name = 'blue'\n",
    "\n",
    "# Visualization and validation sample\n",
    "sample_image = './samples-2021/BraTS2021_00402/BraTS2021_00402_flair.nii.gz' # pick flair modality\n",
    "sample_image_t1 = './samples-2021/BraTS2021_00402/BraTS2021_00402_t1.nii.gz'\n",
    "sample_image_t1ce = './samples-2021/BraTS2021_00402/BraTS2021_00402_t1ce.nii.gz'\n",
    "sample_image_t2 = './samples-2021/BraTS2021_00402/BraTS2021_00402_t2.nii.gz'\n",
    "sample_label = './samples-2021/BraTS2021_00402/BraTS2021_00402_seg.nii.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to AzureML Workspace\n",
    "ml_client = MLClient(DefaultAzureCredential(), subscription_id, resource_group, workspace)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToMultiChannelBasedOnBratsClassesd(MapTransform):\n",
    "    \"\"\"\n",
    "    Convert labels to multi channels based on brats 2021 classes:\n",
    "    label 1 necrotic tumor core (NCR)\n",
    "    label 2 peritumoral edematous/invaded tissue \n",
    "    label 3 is not used in the new dataset version\n",
    "    label 4 GD-enhancing tumor \n",
    "    The possible classes are:\n",
    "      TC (Tumor core): merge labels 1 and 4\n",
    "      WT (Whole tumor): merge labels 1,2 and 4\n",
    "      ET (Enhancing tumor): label 4\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            result = []\n",
    "            # merge label 1 and label 4 to construct TC\n",
    "            result.append(torch.logical_or(d[key] == 1, d[key] == 4))\n",
    "            # merge labels 1, 2 and 4 to construct WT\n",
    "            result.append(\n",
    "                torch.logical_or(\n",
    "                    torch.logical_or(d[key] == 1, d[key] == 2), d[key] == 4\n",
    "                )\n",
    "            )\n",
    "            # label 4 is ET\n",
    "            result.append(d[key] == 4)\n",
    "            d[key] = torch.stack(result, axis=0).float()\n",
    "        return d\n",
    "\n",
    "val_transform = Compose(\n",
    "[\n",
    "    LoadImaged(keys=[\"image\", \"label\"]),\n",
    "    EnsureChannelFirstd(keys=\"image\"),\n",
    "    EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "    ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "    Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "    Spacingd(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        pixdim=(1.0, 1.0, 1.0),\n",
    "        mode=(\"bilinear\", \"nearest\"),\n",
    "    ),\n",
    "    NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "])\n",
    "\n",
    "data_list = [{'image': sample_image, 'label': sample_label}]\n",
    "val_ds = Dataset(data=data_list, transform=val_transform)\n",
    "\n",
    "img_vol = val_ds[0][\"image\"].numpy()\n",
    "seg_vol = val_ds[0][\"label\"].numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_vol.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect 3d structure - viewer works in VSCode \n",
    "\n",
    "img_vol_ch = img_vol[0,:,:,:]\n",
    "seg_vol_ch = seg_vol[0,:,:,:]\n",
    "\n",
    "\n",
    "viewer = view(image= img_vol_ch * 255,\n",
    "              label_image= seg_vol_ch * 255,\n",
    "              gradient_opacity=0.4,\n",
    "              background = (0.5, 0.5, 0.5))\n",
    "              \n",
    "viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_slice(slice_index=64):\n",
    "\n",
    "    img = np.expand_dims(img_vol[0,:,:,slice_index], 0)\n",
    "    seg_ch0 = np.expand_dims(seg_vol[0,:,:,slice_index], 0)\n",
    "    seg_ch1 = np.expand_dims(seg_vol[1,:,:,slice_index], 0)\n",
    "    seg_ch2 = np.expand_dims(seg_vol[2,:,:,slice_index], 0)\n",
    "\n",
    "    # Image with all segmentations overlayed\n",
    "    blend = blend_images(img, seg_ch1, cmap='Blues')\n",
    "    blend = blend_images(blend, seg_ch0, cmap='hsv')\n",
    "    blend = blend_images(blend, seg_ch2, cmap='Greens')\n",
    "    over_all = np.transpose(blend, (1,2,0))\n",
    "\n",
    "    # Individual blends for each segmentation\n",
    "\n",
    "    blend = blend_images(img, seg_ch0, cmap='hsv')\n",
    "    over_ch0 = np.transpose(blend, (1,2,0))\n",
    "    blend = blend_images(img, seg_ch1, cmap='Blues')\n",
    "    over_ch1 = np.transpose(blend, (1,2,0))\n",
    "    blend = blend_images(img, seg_ch2, cmap='Greens')\n",
    "    over_ch2 = np.transpose(blend, (1,2,0))\n",
    "\n",
    "    fig, ((ax1, ax2, ax3, ax4)) = plt.subplots(1, 4, figsize=(24, 8))\n",
    "\n",
    "    ax1.imshow(over_all)\n",
    "    ax1.set_title('All tumor structures')\n",
    "    ax2.imshow(over_ch0)\n",
    "    ax2.set_title('Tumor core')\n",
    "    ax3.imshow(over_ch1)\n",
    "    ax3.set_title('Whole tumor')\n",
    "    ax4.imshow(over_ch2)\n",
    "    ax4.set_title('Enhanceing structure')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Use the interact function to create a slider for the slice index\n",
    "_ = interact(show_slice, slice_index=(0, img_vol.shape[-1]-1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create compute resources, environments and datasets\n",
    "__Note:__ Creating compute resources, training/scoring environments and the dataset __need only performed once__. If you have executed these steps previously, navigate to the next section of this notebook.  \n",
    "\n",
    "Note that we are using low priority compute in this demo as the most cost efficient option. Low priority VMs are significantly cheaper than standard dedictaed compute. However, these resources are not always available and there is a risk that a training job might be pre-empted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    _ = ml_client.compute.get(train_target)\n",
    "    print(\"Found existing compute target.\")\n",
    "except ResourceNotFoundError:\n",
    "    print(\"Creating a new compute target...\")\n",
    "    compute_config = AmlCompute(\n",
    "        name=train_target,\n",
    "        type=\"amlcompute\",\n",
    "        size=\"STANDARD_NC24RS_V3\", # 4 x Tesla V100, 16 GB GPU memory each\n",
    "        tier=\"low_priority\",\n",
    "        idle_time_before_scale_down=600,\n",
    "        min_instances=0,\n",
    "        max_instances=2,\n",
    "    )\n",
    "    ml_client.begin_create_or_update(compute_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_environment = Environment(\n",
    "    image=\"mcr.microsoft.com/azureml/\" + \"openmpi4.1.0-cuda11.1-cudnn8-ubuntu20.04:latest\",\n",
    "    conda_file=\"./src/train-env.yml\",\n",
    "    name=\"monai-multigpu-azureml\",\n",
    "    description=\"Parallel PyTorch training on AzureML with MONAI\")\n",
    "\n",
    "ml_client.environments.create_or_update(training_environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_environment = Environment(\n",
    "    image=\"mcr.microsoft.com/azureml/\" + \"openmpi4.1.0-cuda11.1-cudnn8-ubuntu20.04:latest\",\n",
    "    conda_file=\"./src/scoring-env.yaml\",\n",
    "    name=\"brats-inference-environment\",\n",
    "    description=\"Brain tumor segmentation inference environment\")\n",
    "\n",
    "ml_client.environments.create_or_update(scoring_environment)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The commands below can be used to download the dataset using the Kaggle API (https://github.com/Kaggle/kaggle-api). Use the instructions to generate your own API key and fill them in on the code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Kaggle configuration variables\n",
    "\n",
    "%env KAGGLE_USERNAME=<your Kaggle user name>\n",
    "%env KAGGLE_KEY=<your API token>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and unzip the BRATS dataset.\n",
    "!kaggle datasets download -d dschettler8845/brats-2021-task1 -p /tmp\n",
    "!unzip -q /tmp/brats-2021-task1.zip -d /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the local folder where the dataset will be saved\n",
    "data_dir = \"/tmp/brats\"\n",
    "filename = os.path.join(\"/tmp\", \"BraTS2021_Training_Data.tar\")\n",
    "\n",
    "# Create the local folder if it doesn't exist\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "# Extract the contents of the file to the local folder\n",
    "tar = tarfile.open(filename, \"r\")\n",
    "tar.extractall(path=data_dir)\n",
    "tar.close()\n",
    "\n",
    "print(\"Dataset downloaded and extracted to:\", data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the dataset in the AzureML Workspace\n",
    "\n",
    "my_data = Data(\n",
    "    path=data_dir,\n",
    "    type=AssetTypes.URI_FOLDER,\n",
    "    description=\"Gliomas segmentation necrotic/active tumour and oedema (Source: BRATS 2021 datasets)\",\n",
    "    name=dataset_name,\n",
    ")\n",
    "\n",
    "ml_client.data.create_or_update(my_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Parallel Training Job\n",
    "We are using multi-GPU PyTorch Distrubuted Data Parallel training with scalable Azure ML compute resources. Feel free to change the number of cluster nodes `instance_count` and the number of GPUs per node `process_count_per_instance` to leverage depending on the compute SKU you provisioned.  \n",
    "Note that you can interact with the job for monitoring or debugging using JupyterLab, VSCode, or Tensorboard during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve latest version of BRATS dataset\n",
    "latest_version = [dataset.latest_version for dataset in ml_client.data.list() if dataset.name == dataset_name][0]\n",
    "dataset_asset = ml_client.data.get(name= dataset_name, version= latest_version)\n",
    "print(f'Latest version of {dataset_name}: {latest_version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = command(\n",
    "    inputs = {\"input_data\": Input(type=AssetTypes.URI_FOLDER, path= dataset_asset.path)},\n",
    "    code = 'src/',\n",
    "    command = \"python train-brats21.py --epochs 2 --initial_lr 0.00025 --train_batch_size 1 --val_batch_size 1 --input_data ${{inputs.input_data}} --best_model_name BRATS21\",\n",
    "    environment = \"monai-multigpu-azureml@latest\", \n",
    "    compute = train_target,\n",
    "    experiment_name = experiment,\n",
    "    display_name = f\"3d brain tumor segmentation based on BRATS21\",\n",
    "    description = \"## Brain tumor segmentation on 3D MRI brain scans\",\n",
    "    shm_size='300g',\n",
    "    resources=dict(instance_count= 1), # cluster nodes \n",
    "    distribution=dict(type=\"PyTorch\", process_count_per_instance= 4), # GPUs per node\n",
    "    environment_variables=dict(AZUREML_ARTIFACTS_DEFAULT_TIMEOUT = 1000),\n",
    "    services={\n",
    "    \"My_jupyterlab\": JobService(job_service_type=\"jupyter_lab\"),\n",
    "    \"My_vscode\": JobService(job_service_type=\"vs_code\",),\n",
    "    \"My_tensorboard\": JobService(job_service_type=\"tensor_board\",),\n",
    "        })\n",
    "\n",
    "returned_job = ml_client.create_or_update(job)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model to a Managed Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"MONAI 3d brain tumor segmentation\",\n",
    "    auth_mode=\"key\",\n",
    "    tags={\n",
    "        \"training_dataset\": \"Medical Segmentation Decathlon: Brain tumor segmentation\",\n",
    "        \"model_type\": \"pytorch\",\n",
    "        \"dataset\" : dataset_name,\n",
    "    },\n",
    ")\n",
    "\n",
    "endpoint = ml_client.begin_create_or_update(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = ml_client.online_endpoints.get(online_endpoint_name)\n",
    "print(f\"Endpoint {endpoint.name} provisioning state: {endpoint.provisioning_state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pick the latest version of the model\n",
    "latest_model_version = max([int(m.version) for m in ml_client.models.list(name= registered_model_name)])\n",
    "\n",
    "print(f'Latest version of {registered_model_name} found: {latest_model_version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# picking the model to deploy. Here we use the latest version of our registered model\n",
    "model = ml_client.models.get(name=registered_model_name, version= latest_model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an online deployment.\n",
    "deployment = ManagedOnlineDeployment(\n",
    "    name = deployment_name,\n",
    "    endpoint_name = online_endpoint_name,\n",
    "    model = model,\n",
    "    environment = \"brats-inference-environment@latest\",\n",
    "    code_configuration=CodeConfiguration(code= \"./src\", scoring_script=\"score-brats21.py\"),\n",
    "    instance_type = \"Standard_NC6s_v3\",\n",
    "    instance_count = 1,\n",
    "    request_settings= OnlineRequestSettings(request_timeout_ms = 90000),\n",
    "\n",
    ")\n",
    "deployment = ml_client.begin_create_or_update(deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# existing traffic details\n",
    "print(endpoint.traffic)\n",
    "\n",
    "# Get the scoring URI\n",
    "print(endpoint.scoring_uri)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode input images for JSON request file\n",
    "with open(sample_image, \"rb\") as image_file:\n",
    "    flair_encoded = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "with open(sample_image_t1, \"rb\") as image_file:\n",
    "    t1_encoded = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "with open(sample_image_t1ce, \"rb\") as image_file:\n",
    "    t1ce_encoded = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "with open(sample_image_t2, \"rb\") as image_file:\n",
    "    t2_encoded = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "request_data = {\n",
    "    \"data\": [{\"flair\": flair_encoded, \"t1\": t1_encoded, \n",
    "              \"t1ce\": t1ce_encoded, \"t2\": t2_encoded\n",
    "             }]\n",
    "}\n",
    "\n",
    "# Write the JSON request data to a file\n",
    "with open(\"request-brats2021.json\", \"w\") as outfile:\n",
    "    json.dump(request_data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send request to Managed Online Endpoint\n",
    "response = ml_client.online_endpoints.invoke(\n",
    "    endpoint_name= online_endpoint_name,\n",
    "    deployment_name= deployment_name,\n",
    "    request_file=\"./request-brats2021.json\",\n",
    ")\n",
    "\n",
    "# convert response to numpy array with dimensions channel, height, width, slice\n",
    "json_response = json.loads(response)\n",
    "pred_vol = np.array(json_response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Predictions\n",
    "We are inspecting the predictions for the tumor core segmentations and compare them with the ground truth annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect 3d structure - viewer works in VSCode \n",
    "\n",
    "img_vol_ch = img_vol[0,:,:,:]\n",
    "seg_vol_ch = pred_vol[0,:,:,:]\n",
    "\n",
    "viewer = view(image= img_vol_ch * 255,\n",
    "              label_image= seg_vol_ch * 255,\n",
    "              gradient_opacity=0.4,\n",
    "              background = (0.5, 0.5, 0.5))\n",
    "              \n",
    "viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_slice(slice_index=60):\n",
    "\n",
    "    img = np.expand_dims(img_vol[0,:,:,slice_index], 0) # images\n",
    "    true_seg = np.expand_dims(seg_vol[0,:,:,slice_index], 0) # annotated ground truth labels\n",
    "    pred_seg = np.expand_dims(pred_vol[0,:,:,slice_index], 0)  # predicted labels\n",
    "    \n",
    "    blend = blend_images(img, true_seg, cmap='hsv')\n",
    "    over_true = np.transpose(blend, (1,2,0))\n",
    "    blend = blend_images(img, pred_seg, cmap='Blues')\n",
    "    over_pred = np.transpose(blend, (1,2,0))\n",
    "    \n",
    "    fig, ((ax1, ax2)) = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "    ax1.imshow(over_true)\n",
    "    ax1.set_title('Ground truth segmentations')\n",
    "    ax2.imshow(over_pred)\n",
    "    ax2.set_title('Predicted segmentations')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Use the interact function to create a slider for the slice index\n",
    "_ = interact(show_slice, slice_index=(0, img_vol.shape[-1]-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
